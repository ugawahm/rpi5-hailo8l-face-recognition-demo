import cv2
import numpy as np
import degirum as dg
import time
import threading
from collections import OrderedDict
from queue import Queue
from datetime import datetime
from pathlib import Path

from input_handler import InputHandler
from face_database import FaceDatabase
from quality_assessment import QualityAssessor
from utils import bbox_iou, get_quality_indicator_color, draw_help_text

class FaceRecognitionSystem:
    def __init__(self, faces_dir="./faces", models_dir="./models", detect_every_n_frames=5, 
                 cache_size=10, input_source="rpi", confidence_threshold=0.55, 
                 adaptive_threshold=True):
        """åˆå§‹åŒ–ç¨‹åº
        
        Args:
            input_source: è¾“å…¥æº ('rpi', 'usb', æˆ–æ–‡ä»¶è·¯å¾„)
            confidence_threshold: äººè„¸è¯†åˆ«ç½®ä¿¡åº¦é˜ˆå€¼
            adaptive_threshold: æ˜¯å¦å¯ç”¨è‡ªé€‚åº”é˜ˆå€¼
        """
        print("ğŸš€ åˆå§‹åŒ–...")
        
        # åˆå§‹åŒ– DeGirum æœ¬åœ°æ¨¡å‹Zoo
        print(f"ğŸ“¦ è¿æ¥æœ¬åœ°æ¨¡å‹Zoo: {models_dir}")
        zoo = dg.connect(dg.LOCAL, models_dir)
        
        # åŠ è½½äººè„¸æ£€æµ‹æ¨¡å‹
        print("ğŸ” åŠ è½½äººè„¸æ£€æµ‹æ¨¡å‹...")
        self.face_detector = zoo.load_model("scrfd_10g--640x640_quant_hailort_hailo8l_1")
        self.face_detector.input_letterbox_fill_color = (114, 114, 114)
        print("âœ… äººè„¸æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åŠ è½½äººè„¸ç‰¹å¾æå–æ¨¡å‹
        print("ğŸ§¬ åŠ è½½äººè„¸ç‰¹å¾æå–æ¨¡å‹...")
        self.face_encoder = zoo.load_model("arcface_r50")
        print("âœ… äººè„¸ç‰¹å¾æå–æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åˆå§‹åŒ–è¾“å…¥æº
        self.input_handler = InputHandler(input_source)
        self.input_source_type = input_source
        
        # äººè„¸åº“ç®¡ç†
        self.face_database_manager = FaceDatabase(faces_dir, self.face_detector, self.face_encoder)
        self.face_database_manager.load_face_database()
        
        # æ€§èƒ½ä¼˜åŒ–å‚æ•°
        self.detect_every_n_frames = detect_every_n_frames
        self.frame_counter = 0
        self.last_detections = []
        self.last_captured_faces = []
        self.base_threshold = confidence_threshold
        self.adaptive_threshold_enabled = adaptive_threshold
        
        # å›¾åƒè´¨é‡è¯„ä¼°å’Œè‡ªé€‚åº”é˜ˆå€¼
        self.quality_assessor = QualityAssessor(confidence_threshold, adaptive_threshold)
        
        # ç‰¹å¾ç¼“å­˜
        self.feature_cache = OrderedDict()
        self.cache_size = cache_size
        
        # å¤šçº¿ç¨‹é˜Ÿåˆ—
        self.detection_queue = Queue(maxsize=2)
        self.recognition_queue = Queue(maxsize=5)
        self.result_queue = Queue(maxsize=5)
        
        # çº¿ç¨‹æ§åˆ¶
        self.running = True
        self.detection_thread = None
        self.recognition_thread = None
        
        # FPS è®¡ç®—
        self.fps = 0
        self.frame_count = 0
        self.start_time = time.time()
        
        # è¾“å…¥æ¨¡å¼æ§åˆ¶
        self.input_mode = False
        
        # å¯åŠ¨åå°çº¿ç¨‹
        self._start_threads()

    def _start_threads(self):
        """å¯åŠ¨åå°å¤„ç†çº¿ç¨‹"""
        print("ğŸ”§ å¯åŠ¨å¤šçº¿ç¨‹å¤„ç†...")
        
        # æ£€æµ‹çº¿ç¨‹
        self.detection_thread = threading.Thread(target=self._detection_worker, daemon=True)
        self.detection_thread.start()
        
        # è¯†åˆ«çº¿ç¨‹
        self.recognition_thread = threading.Thread(target=self._recognition_worker, daemon=True)
        self.recognition_thread.start()
        
        print("âœ… å¤šçº¿ç¨‹å¯åŠ¨æˆåŠŸ")

    def _detection_worker(self):
        """åå°äººè„¸æ£€æµ‹çº¿ç¨‹"""
        while self.running:
            try:
                # ä»é˜Ÿåˆ—è·å–å¸§
                if self.detection_queue.empty():
                    time.sleep(0.001)
                    continue
                
                frame_data = self.detection_queue.get(timeout=0.1)
                if frame_data is None:
                    continue
                
                frame, frame_id = frame_data
                
                # æ‰§è¡Œäººè„¸æ£€æµ‹
                result = self.face_detector(frame)
                detected_faces = []
                
                if hasattr(result, 'results') and len(result.results) > 0:
                    for detection in result.results:
                        try:
                            bbox = detection['bbox']
                            x1, y1, x2, y2 = map(int, bbox)
                            h, w = frame.shape[:2]
                            x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w, x2), min(h, y2)
                            
                            if x2 <= x1 or y2 <= y1:
                                continue
                            
                            face_crop = frame[y1:y2, x1:x2]
                            if face_crop.size == 0:
                                continue
                            
                            # è¯„ä¼°å›¾åƒè´¨é‡
                            quality = self.quality_assessor.assess_image_quality(frame, (x1, y1, x2, y2))
                            
                            detected_faces.append({
                                'bbox': (x1, y1, x2, y2),
                                'crop': face_crop,
                                'quality': quality
                            })
                        except Exception as e:
                            # print(f"Error processing detection: {e}")
                            continue
                
                # å°†æ£€æµ‹ç»“æœå‘é€åˆ°è¯†åˆ«é˜Ÿåˆ—
                if not self.recognition_queue.full():
                    self.recognition_queue.put((detected_faces, frame_id))
                
            except Exception as e:
                # print(f"Detection worker error: {e}")
                continue

    def _recognition_worker(self):
        """åå°äººè„¸è¯†åˆ«çº¿ç¨‹"""
        while self.running:
            try:
                if self.recognition_queue.empty():
                    time.sleep(0.001)
                    continue
                
                face_data = self.recognition_queue.get(timeout=0.1)
                if face_data is None:
                    continue
                
                detected_faces, frame_id = face_data
                recognition_results = []
                
                for face_info in detected_faces:
                    bbox = face_info['bbox']
                    face_crop = face_info['crop']
                    quality = face_info['quality']
                    
                    # æå–ç‰¹å¾
                    feature_vector = self.extract_features_with_cache(face_crop, bbox)
                    if feature_vector is None:
                        continue
                    
                    # è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼
                    adaptive_threshold = self.quality_assessor.calculate_adaptive_threshold(quality)
                    
                    # è¯†åˆ«äººè„¸
                    name, similarity = self.face_database_manager.recognize_face(feature_vector, adaptive_threshold)
                    confidence = similarity * 100
                    
                    recognition_results.append({
                        'bbox': bbox,
                        'name': name,
                        'confidence': confidence,
                        'quality': quality,
                        'threshold': adaptive_threshold,
                        'crop': face_crop,
                        'features': feature_vector
                    })
                
                # å°†ç»“æœå‘é€åˆ°ç»“æœé˜Ÿåˆ—
                if not self.result_queue.full():
                    self.result_queue.put((recognition_results, frame_id))
                
            except Exception as e:
                # print(f"Recognition worker error: {e}")
                continue

    def extract_features_with_cache(self, face_crop, bbox, iou_threshold=0.8):
        """ä½¿ç”¨ç¼“å­˜æå–ç‰¹å¾"""
        bbox_tuple = tuple(bbox)
        
        for cached_bbox, cached_features in list(self.feature_cache.items()):
            if bbox_iou(bbox_tuple, cached_bbox) > iou_threshold:
                self.feature_cache.move_to_end(cached_bbox)
                return cached_features

        features = self.face_database_manager.extract_features(face_crop)
        if features is not None:
            if len(self.feature_cache) >= self.cache_size:
                self.feature_cache.popitem(last=False)
            self.feature_cache[bbox_tuple] = features
        return features
    
    def update_fps(self):
        """æ›´æ–°FPS"""
        self.frame_count += 1
        elapsed_time = time.time() - self.start_time
        
        if elapsed_time >= 1.0:
            self.fps = self.frame_count / elapsed_time
            self.frame_count = 0
            self.start_time = time.time()

    def handle_face_capture_async(self, captured_faces):
        """å¼‚æ­¥å¤„ç†äººè„¸æ•è·è¾“å…¥"""
        if not captured_faces:
            print("âŒ æœªæ£€æµ‹åˆ°äººè„¸ï¼Œè¯·å¯¹å‡†æ‘„åƒå¤´")
            self.input_mode = False
            return

        print(f"\nğŸ“¸ æ£€æµ‹åˆ° {len(captured_faces)} ä¸ªäººè„¸")

        if len(captured_faces) == 1:
            print("è¯·è¾“å…¥å§“å (ç›´æ¥å›è½¦å–æ¶ˆ): ", end='', flush=True)
            try:
                name = input().strip()
            except (EOFError, KeyboardInterrupt):
                print("\nâŒ è¾“å…¥è¢«ä¸­æ–­")
                self.input_mode = False
                return

            if name:
                face_info = captured_faces[0]
                if self.face_database_manager.save_face_to_database(
                    face_info['crop'], 
                    face_info['features'], 
                    name
                ):
                    print(f"âœ… æˆåŠŸæ·»åŠ  {name} åˆ°äººè„¸åº“!")
            else:
                print("âŒ å·²å–æ¶ˆ")
        else:
            print("è¯·é€‰æ‹©è¦ä¿å­˜çš„äººè„¸:")
            for i in range(len(captured_faces)):
                print(f"  [{i+1}] äººè„¸ {i+1}")
            print("  [0] å–æ¶ˆ")

            try:
                choice_input = input("é€‰æ‹© (0-{}): ".format(len(captured_faces))).strip()
                if not choice_input:
                    print("âŒ å·²å–æ¶ˆ")
                    self.input_mode = False
                    return
                choice = int(choice_input)

                if 1 <= choice <= len(captured_faces):
                    print("è¯·è¾“å…¥å§“å (ç›´æ¥å›è½¦å–æ¶ˆ): ", end='', flush=True)
                    name = input().strip()

                    if name:
                        face_info = captured_faces[choice - 1]
                        if self.face_database_manager.save_face_to_database(
                            face_info['crop'], 
                            face_info['features'], 
                            name
                        ):
                            print(f"âœ… æˆåŠŸæ·»åŠ  {name} åˆ°äººè„¸åº“!")
                    else:
                        print("âŒ å·²å–æ¶ˆ")
                else:
                    print("âŒ å·²å–æ¶ˆ")
            except (ValueError, EOFError, KeyboardInterrupt):
                print("âŒ æ“ä½œå·²å–æ¶ˆ")

        print()
        self.input_mode = False

    def run(self):
        """è¿è¡Œä¸»å¾ªç¯"""
        print("\nğŸ¬ å¼€å§‹å®æ—¶äººè„¸è¯†åˆ«...")
        print("=" * 60)
        print("âŒ¨ï¸  æŒ‰é”®è¯´æ˜:")
        print("   's'   - æ•è·å½“å‰äººè„¸å¹¶æ·»åŠ åˆ°æ•°æ®åº“")
        print("   'ESC' - å–æ¶ˆè¾“å…¥ (åœ¨è¾“å…¥å§“åæ—¶)")
        print("   'q'   - é€€å‡ºç¨‹åº")
        print("=" * 60)
        print(f"ğŸ“¹ è¾“å…¥æº: {self.input_source_type}")
        print(f"âš™ï¸  è·³å¸§æ£€æµ‹: æ¯ {self.detect_every_n_frames} å¸§æ‰§è¡Œä¸€æ¬¡æ£€æµ‹")
        print(f"ğŸ¯ åŸºå‡†é˜ˆå€¼: {self.base_threshold:.2f}")
        print(f"ğŸ§  è‡ªé€‚åº”é˜ˆå€¼: {'å¯ç”¨' if self.adaptive_threshold_enabled else 'ç¦ç”¨'}")
        print(f"ğŸ”§ å¤šçº¿ç¨‹å¤„ç†: å¯ç”¨")
        print()
        
        frame_id = 0
        
        try:
            while True:
                loop_start = time.time()
                frame = self.input_handler.capture_frame()
                
                if frame is None:
                    print("âŒ æ— æ³•è·å–å¸§")
                    break
                
                self.frame_counter += 1
                frame_id += 1
                
                # å®šæœŸå‘æ£€æµ‹é˜Ÿåˆ—å‘é€å¸§
                if self.frame_counter % self.detect_every_n_frames == 0:
                    if not self.detection_queue.full():
                        self.detection_queue.put((frame.copy(), frame_id))
                
                # ä»ç»“æœé˜Ÿåˆ—è·å–è¯†åˆ«ç»“æœ
                current_detections = []
                captured_faces_for_save = []
                
                if not self.result_queue.empty():
                    try:
                        results, result_frame_id = self.result_queue.get_nowait()
                        
                        for result in results:
                            bbox = result['bbox']
                            name = result['name']
                            confidence = result['confidence']
                            quality = result['quality']
                            
                            current_detections.append((bbox, name, confidence, quality))
                            
                            captured_faces_for_save.append({
                                'bbox': bbox,
                                'crop': result['crop'],
                                'features': result['features']
                            })
                        
                        self.last_detections = current_detections
                        self.last_captured_faces = captured_faces_for_save
                        
                    except:
                        current_detections = self.last_detections
                else:
                    current_detections = self.last_detections

                # ç»˜åˆ¶æ£€æµ‹æ¡†å’Œä¿¡æ¯
                for bbox, name, confidence, quality in current_detections:
                    x1, y1, x2, y2 = bbox
                    
                    # æ ¹æ®è¯†åˆ«ç»“æœé€‰æ‹©é¢œè‰²
                    if name != "Unknown":
                        color_rgb = (0, 255, 0)
                    else:
                        color_rgb = (255, 0, 0)
                    
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color_rgb, 2)
                    
                    # æ˜¾ç¤ºè¯†åˆ«ç»“æœ
                    label = f"{name}: {confidence:.1f}%"
                    cv2.putText(frame, label, (x1, y1 - 10),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color_rgb, 2)
                    
                    # æ˜¾ç¤ºè´¨é‡æŒ‡æ ‡ï¼ˆè°ƒè¯•ä¿¡æ¯ï¼‰
                    quality_color = get_quality_indicator_color(quality)
                    quality_text = f"Q: B{quality['brightness']:.0f} C{quality['contrast']:.0f} S{quality['blur']:.0f}"
                    cv2.putText(frame, quality_text, (x1, y2 + 20),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, quality_color, 1)

                if self.input_mode:
                    mode_text = "Waiting for input... (Check terminal)"
                    cv2.putText(frame, mode_text, (10, frame.shape[0] - 20),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)

                # æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
                self.update_fps()
                cv2.putText(frame, f"FPS: {self.fps:.1f}", (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
                cv2.putText(frame, f"Database: {len(self.face_database_manager.face_database)} faces", 
                           (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                cv2.putText(frame, f"Detected: {len(current_detections)} face(s)", 
                           (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                # æ˜¾ç¤ºè‡ªé€‚åº”é˜ˆå€¼ä¿¡æ¯
                if self.adaptive_threshold_enabled:
                    threshold_text = f"Threshold: {self.quality_assessor.current_threshold:.3f} (Base: {self.base_threshold:.2f})"
                    cv2.putText(frame, threshold_text, (10, 120),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 0), 1)

                draw_help_text(frame)

                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                cv2.imshow("Face Recognition System - Multi-threaded", frame_bgr)

                # åŠ¨æ€è°ƒæ•´å»¶è¿Ÿ
                elapsed = time.time() - loop_start
                target_fps = 24
                delay = max(1, int((1.0 / target_fps - elapsed) * 1000))
                key = cv2.waitKey(delay) & 0xFF

                if key == ord('q'):
                    break
                elif key == ord('s') and not self.input_mode:
                    captured_faces_snapshot = self.last_captured_faces.copy()
                    if not captured_faces_snapshot:
                        print("âŒ æœªæ£€æµ‹åˆ°äººè„¸ï¼Œè¯·å¯¹å‡†æ‘„åƒå¤´")
                        continue

                    self.input_mode = True
                    thread = threading.Thread(
                        target=self.handle_face_capture_async,
                        args=(captured_faces_snapshot,)
                    )
                    thread.daemon = True
                    thread.start()

                # é™æ€å›¾ç‰‡æ¨¡å¼
                if self.input_handler.is_static_image_source():
                    key = cv2.waitKey(0) & 0xFF
                    if key == ord('q'):
                        break

        except KeyboardInterrupt:
            print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­")
        finally:
            self.cleanup()
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        print("ğŸ§¹ æ¸…ç†èµ„æº...")
        self.running = False
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        if self.detection_thread and self.detection_thread.is_alive():
            self.detection_thread.join(timeout=1)
        if self.recognition_thread and self.recognition_thread.is_alive():
            self.recognition_thread.join(timeout=1)
        
        self.input_handler.release()
        cv2.destroyAllWindows()
        print("âœ… å®Œæˆ")
