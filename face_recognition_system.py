import cv2
import numpy as np
import degirum as dg
import time
import threading
from collections import OrderedDict
from queue import Queue
from datetime import datetime
from pathlib import Path

from input_handler import InputHandler
from face_database import FaceDatabase
from quality_assessment import QualityAssessor
from utils import bbox_iou, get_quality_indicator_color, draw_help_text

class FaceRecognitionSystem:
    def __init__(self, faces_dir="faces", models_dir="./models", detect_every_n_frames=3, 
                 cache_size=10, input_source="rpi", confidence_threshold=0.6, 
                 adaptive_threshold=True):
        """åˆå§‹åŒ–äººè„¸è¯†åˆ«ç³»ç»Ÿ
        
        Args:
            input_source: è¾“å…¥æº ('rpi', 'usb', æˆ–æ–‡ä»¶è·¯å¾„)
            confidence_threshold: äººè„¸è¯†åˆ«ç½®ä¿¡åº¦é˜ˆå€¼
            adaptive_threshold: æ˜¯å¦å¯ç”¨è‡ªé€‚åº”é˜ˆå€¼
        """
        init_start_time = time.time()
        print("ğŸš€ åˆå§‹åŒ–äººè„¸è¯†åˆ«ç³»ç»Ÿ...")
        
        # åˆå§‹åŒ– DeGirum æœ¬åœ°æ¨¡å‹Zoo
        print(f"ğŸ“¦ è¿æ¥æœ¬åœ°æ¨¡å‹Zoo: {models_dir}")
        zoo_connect_start = time.time()
        zoo = dg.connect(dg.LOCAL, models_dir)
        print(f"âœ… è¿æ¥æœ¬åœ°æ¨¡å‹ZooæˆåŠŸï¼Œè€—æ—¶: {time.time() - zoo_connect_start:.2f}s")
        
        # åŠ è½½äººè„¸æ£€æµ‹æ¨¡å‹
        print("ğŸ” åŠ è½½äººè„¸æ£€æµ‹æ¨¡å‹...")
        detector_load_start = time.time()
        self.face_detector = zoo.load_model("scrfd_10g--640x640_quant_hailort_hailo8l_1")
        self.face_detector.input_letterbox_fill_color = (114, 114, 114)
        print(f"âœ… äººè„¸æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè€—æ—¶: {time.time() - detector_load_start:.2f}s")
        
        # åŠ è½½äººè„¸ç‰¹å¾æå–æ¨¡å‹
        print("ğŸ§¬ åŠ è½½äººè„¸ç‰¹å¾æå–æ¨¡å‹...")
        encoder_load_start = time.time()
        self.face_encoder = zoo.load_model("arcface_r50")
        print(f"âœ… äººè„¸ç‰¹å¾æå–æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè€—æ—¶: {time.time() - encoder_load_start:.2f}s")
        
        # åˆå§‹åŒ–è¾“å…¥æº
        print("ğŸ¥ åˆå§‹åŒ–è¾“å…¥æº...")
        input_handler_init_start = time.time()
        self.input_handler = InputHandler(input_source)
        self.input_source_type = input_source
        print(f"âœ… è¾“å…¥æºåˆå§‹åŒ–æˆåŠŸï¼Œè€—æ—¶: {time.time() - input_handler_init_start:.2f}s")
        
        # äººè„¸åº“ç®¡ç†
        print("ğŸ“š åˆå§‹åŒ–äººè„¸æ•°æ®åº“ç®¡ç†å™¨...")
        db_manager_init_start = time.time()
        self.face_database_manager = FaceDatabase(faces_dir, self.face_detector, self.face_encoder)
        self.face_database_manager.load_face_database() # è®¡æ—¶å·²åœ¨FaceDatabaseå†…éƒ¨
        print(f"âœ… äººè„¸æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸï¼Œè€—æ—¶: {time.time() - db_manager_init_start:.2f}s")
        
        # æ€§èƒ½ä¼˜åŒ–å‚æ•°
        self.detect_every_n_frames = detect_every_n_frames 
        self.frame_counter = 0
        self.last_detections = []
        self.last_captured_faces = []
        self.base_threshold = confidence_threshold
        self.adaptive_threshold_enabled = adaptive_threshold
        
        # å›¾åƒè´¨é‡è¯„ä¼°å’Œè‡ªé€‚åº”é˜ˆå€¼
        self.quality_assessor = QualityAssessor(confidence_threshold, adaptive_threshold)
        
        # ç‰¹å¾ç¼“å­˜
        self.feature_cache = OrderedDict()
        self.cache_size = cache_size
        
        # å¤šçº¿ç¨‹é˜Ÿåˆ—
        self.detection_queue = Queue(maxsize=2)
        self.recognition_queue = Queue(maxsize=5)
        self.result_queue = Queue(maxsize=5)
        
        # çº¿ç¨‹æ§åˆ¶
        self.running = True
        self.detection_thread = None
        self.recognition_thread = None
        
        # FPS è®¡ç®—
        self.fps = 0
        self.frame_count = 0
        self.start_time = time.time()
        
        # è¾“å…¥æ¨¡å¼æ§åˆ¶
        self.input_mode = False

        # é¦–æ¬¡è°ƒç”¨æ¨¡å‹æ ‡è®°
        # åœ¨çƒ­èº«åï¼Œå°†è¿™äº›è®¾ç½®ä¸º Falseï¼Œè¿™æ ·ä¸»å¾ªç¯ä¸­çš„ç¬¬ä¸€æ¬¡çœŸå®æ£€æµ‹å°±ä¸ä¼šå†æ‰“å°â€œé¦–å¸§è€—æ—¶â€
        self.first_detection_run = False 
        self.first_recognition_run = False 
        
        # å¯åŠ¨åå°çº¿ç¨‹
        self._start_threads()

        # --- æ–°å¢ï¼šæ¨¡å‹çƒ­èº« (Model Warm-up) ---
        print("ğŸ”¥ å¯¹Hailo-8Læ¨¡å‹è¿›è¡Œçƒ­èº«...")
        warmup_start = time.time()
        
        # è·å–æ‘„åƒå¤´çš„é»˜è®¤åˆ†è¾¨ç‡ï¼Œç”¨äºåˆ›å»ºå‡çš„ç©ºç™½å¸§
        # å‡è®¾InputHandlerå·²ç»åˆå§‹åŒ–å¹¶å¯ä»¥è·å–åˆ†è¾¨ç‡
        # ä½ å¯èƒ½éœ€è¦æ ¹æ®ä½ çš„InputHandlerå®ç°æ¥è·å–æ­£ç¡®çš„å°ºå¯¸
        # å¦‚æœæ˜¯è§†é¢‘æ–‡ä»¶ï¼Œå¯ä»¥å°è¯•ä»è§†é¢‘è¯»å–ç¬¬ä¸€å¸§æ¥è·å–å°ºå¯¸
        dummy_width = 640  # é»˜è®¤å€¼ï¼Œè¯·æ ¹æ®ä½ çš„å®é™…æƒ…å†µè°ƒæ•´
        dummy_height = 480 # é»˜è®¤å€¼ï¼Œè¯·æ ¹æ®ä½ çš„å®é™…æƒ…å†µè°ƒæ•´
        
        try:
            if self.input_source_type == "rpi":
                # Picamera2 å¯åŠ¨ååˆ†è¾¨ç‡ä¼šç¨³å®š
                # å¯ä»¥åœ¨ InputHandler ä¸­æ·»åŠ æ–¹æ³•æ¥è·å–å½“å‰åˆ†è¾¨ç‡
                # æˆ–è€…ç›´æ¥ä½¿ç”¨ä¸€ä¸ªå·²çŸ¥çš„åˆ†è¾¨ç‡ï¼Œæ¯”å¦‚ 640x480
                pass # ä¿æŒé»˜è®¤ dummy_width/height æˆ–ä» input_handler è·å–
            elif self.input_source_type == "usb":
                 # USBæ‘„åƒå¤´å¯èƒ½é»˜è®¤åˆ†è¾¨ç‡
                 pass # ä¿æŒé»˜è®¤ dummy_width/height æˆ–ä» input_handler è·å–
            else: # è§†é¢‘æ–‡ä»¶
                temp_cap = cv2.VideoCapture(self.input_source_type)
                if temp_cap.isOpened():
                    dummy_width = int(temp_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                    dummy_height = int(temp_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                    temp_cap.release()
                else:
                    print(f"   âš ï¸ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶ {self.input_source_type} è·å–åˆ†è¾¨ç‡ï¼Œä½¿ç”¨é»˜è®¤ {dummy_width}x{dummy_height}")
        except Exception as e:
            print(f"   âš ï¸ è·å–è¾“å…¥æºåˆ†è¾¨ç‡å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤ {dummy_width}x{dummy_height}")

        dummy_frame = np.zeros((dummy_height, dummy_width, 3), dtype=np.uint8) 
        
        # 1. çƒ­èº«äººè„¸æ£€æµ‹æ¨¡å‹
        try:
            print("   -> çƒ­èº«äººè„¸æ£€æµ‹æ¨¡å‹...")
            _ = self.face_detector(dummy_frame)
            print("   âœ… äººè„¸æ£€æµ‹æ¨¡å‹çƒ­èº«å®Œæˆã€‚")
        except Exception as e:
            print(f"   âŒ äººè„¸æ£€æµ‹æ¨¡å‹çƒ­èº«å¤±è´¥: {e}")
            
        # 2. çƒ­èº«äººè„¸ç‰¹å¾æå–æ¨¡å‹ (éœ€è¦è£å‰ªå‡ºâ€œå‡â€çš„äººè„¸åŒºåŸŸ)
        # ç¡®ä¿è£å‰ªåŒºåŸŸä¸ä¸ºç©ºï¼Œä¸”è¶³å¤Ÿå¤§ä¾›æ¨¡å‹å¤„ç†
        dummy_face_crop_h = min(100, dummy_height)
        dummy_face_crop_w = min(100, dummy_width)
        dummy_face_crop = dummy_frame[0:dummy_face_crop_h, 0:dummy_face_crop_w] 

        if dummy_face_crop.size > 0 and dummy_face_crop_h > 0 and dummy_face_crop_w > 0:
            try:
                print("   -> çƒ­èº«äººè„¸ç‰¹å¾æå–æ¨¡å‹...")
                _ = self.face_encoder(dummy_face_crop)
                print("   âœ… äººè„¸ç‰¹å¾æå–æ¨¡å‹çƒ­èº«å®Œæˆã€‚")
            except Exception as e:
                print(f"   âŒ äººè„¸ç‰¹å¾æå–æ¨¡å‹çƒ­èº«å¤±è´¥: {e}")
        else:
            print("   âš ï¸ æ— æ³•åˆ›å»ºæœ‰æ•ˆå‡äººè„¸åŒºåŸŸï¼Œè·³è¿‡ç‰¹å¾æå–æ¨¡å‹çƒ­èº«ã€‚")

        print(f"ğŸ”¥ æ¨¡å‹çƒ­èº«æ€»è€—æ—¶: {time.time() - warmup_start:.2f}s")
        # --- çƒ­èº«ç»“æŸ ---

        print(f"ğŸš€ äººè„¸è¯†åˆ«ç³»ç»Ÿåˆå§‹åŒ–æ€»è€—æ—¶: {time.time() - init_start_time:.2f}s")


    def _start_threads(self):
        """å¯åŠ¨åå°å¤„ç†çº¿ç¨‹"""
        print("ğŸ”§ å¯åŠ¨å¤šçº¿ç¨‹å¤„ç†...")
        
        # æ£€æµ‹çº¿ç¨‹
        self.detection_thread = threading.Thread(target=self._detection_worker, daemon=True)
        self.detection_thread.start()
        
        # è¯†åˆ«çº¿ç¨‹
        self.recognition_thread = threading.Thread(target=self._recognition_worker, daemon=True)
        self.recognition_thread.start()
        
        print("âœ… å¤šçº¿ç¨‹å¯åŠ¨æˆåŠŸ")

    def _detection_worker(self):
        """åå°äººè„¸æ£€æµ‹çº¿ç¨‹"""
        while self.running:
            try:
                # ä»é˜Ÿåˆ—è·å–å¸§
                if self.detection_queue.empty():
                    time.sleep(0.001)
                    continue
                
                frame_data = self.detection_queue.get(timeout=0.1)
                if frame_data is None:
                    continue
                
                frame, frame_id = frame_data
                
                # æ‰§è¡Œäººè„¸æ£€æµ‹
                detection_start = time.time()
                result = self.face_detector(frame)
                
                # if self.first_detection_run:
                #     print(f"â±ï¸ é¦–å¸§äººè„¸æ£€æµ‹è€—æ—¶: {time.time() - detection_start:.3f}s (çƒ­èº«å)")
                #     self.first_detection_run = False
                
                detected_faces = []
                
                if hasattr(result, 'results') and len(result.results) > 0:
                    for detection in result.results:
                        try:
                            bbox = detection['bbox']
                            x1, y1, x2, y2 = map(int, bbox)
                            h, w = frame.shape[:2]
                            x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w, x2), min(h, y2)
                            
                            if x2 <= x1 or y2 <= y1:
                                continue
                            
                            face_crop = frame[y1:y2, x1:x2]
                            if face_crop.size == 0:
                                continue
                            
                            # è¯„ä¼°å›¾åƒè´¨é‡
                            quality = self.quality_assessor.assess_image_quality(frame, (x1, y1, x2, y2))
                            
                            detected_faces.append({
                                'bbox': (x1, y1, x2, y2),
                                'crop': face_crop,
                                'quality': quality
                            })
                        except Exception as e:
                            # print(f"Error processing detection: {e}")
                            continue
                
                # å°†æ£€æµ‹ç»“æœå‘é€åˆ°è¯†åˆ«é˜Ÿåˆ—
                if not self.recognition_queue.full():
                    self.recognition_queue.put((detected_faces, frame_id))
                
            except Exception as e:
                # print(f"Detection worker error: {e}")
                continue

    def _recognition_worker(self):
        """åå°äººè„¸è¯†åˆ«çº¿ç¨‹"""
        while self.running:
            try:
                if self.recognition_queue.empty():
                    time.sleep(0.001)
                    continue
                
                face_data = self.recognition_queue.get(timeout=0.1)
                if face_data is None:
                    continue
                
                detected_faces, frame_id = face_data
                recognition_results = []
                
                for face_info in detected_faces:
                    bbox = face_info['bbox']
                    face_crop = face_info['crop']
                    quality = face_info['quality']
                    
                    # æå–ç‰¹å¾
                    encode_start = time.time()
                    feature_vector = self.extract_features_with_cache(face_crop, bbox)
                    # if self.first_recognition_run:
                    #      print(f"â±ï¸ é¦–å¸§äººè„¸ç‰¹å¾æå–è€—æ—¶: {time.time() - encode_start:.3f}s (çƒ­èº«å)")
                    #      self.first_recognition_run = False
                    
                    if feature_vector is None:
                        continue
                    
                    # è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼
                    adaptive_threshold = self.quality_assessor.calculate_adaptive_threshold(quality)
                    
                    # è¯†åˆ«äººè„¸
                    name, similarity = self.face_database_manager.recognize_face(feature_vector, adaptive_threshold)
                    confidence = similarity * 100
                    
                    recognition_results.append({
                        'bbox': bbox,
                        'name': name,
                        'confidence': confidence,
                        'quality': quality,
                        'threshold': adaptive_threshold,
                        'crop': face_crop,
                        'features': feature_vector
                    })
                
                # å°†ç»“æœå‘é€åˆ°ç»“æœé˜Ÿåˆ—
                if not self.result_queue.full():
                    self.result_queue.put((recognition_results, frame_id))
                
            except Exception as e:
                # print(f"Recognition worker error: {e}")
                continue

    def extract_features_with_cache(self, face_crop, bbox, iou_threshold=0.8):
        """ä½¿ç”¨ç¼“å­˜æå–ç‰¹å¾"""
        bbox_tuple = tuple(bbox)
        
        for cached_bbox, cached_features in list(self.feature_cache.items()):
            if bbox_iou(bbox_tuple, cached_bbox) > iou_threshold:
                self.feature_cache.move_to_end(cached_bbox)
                return cached_features

        features = self.face_database_manager.extract_features(face_crop)
        if features is not None:
            if len(self.feature_cache) >= self.cache_size:
                self.feature_cache.popitem(last=False)
            self.feature_cache[bbox_tuple] = features
        return features
    
    def update_fps(self):
        """æ›´æ–°FPS"""
        self.frame_count += 1
        elapsed_time = time.time() - self.start_time
        
        if elapsed_time >= 1.0:
            self.fps = self.frame_count / elapsed_time
            self.frame_count = 0
            self.start_time = time.time()

    def handle_face_capture_async(self, captured_faces):
        """å¼‚æ­¥å¤„ç†äººè„¸æ•è·è¾“å…¥"""
        if not captured_faces:
            print("âŒ æœªæ£€æµ‹åˆ°äººè„¸ï¼Œè¯·å¯¹å‡†æ‘„åƒå¤´")
            self.input_mode = False
            return

        print(f"\nğŸ“¸ æ£€æµ‹åˆ° {len(captured_faces)} ä¸ªäººè„¸")

        if len(captured_faces) == 1:
            print("è¯·è¾“å…¥å§“å (ç›´æ¥å›è½¦å–æ¶ˆ): ", end='', flush=True)
            try:
                name = input().strip()
            except (EOFError, KeyboardInterrupt):
                print("\nâŒ è¾“å…¥è¢«ä¸­æ–­")
                self.input_mode = False
                return

            if name:
                face_info = captured_faces[0]
                if self.face_database_manager.save_face_to_database(
                    face_info['crop'], 
                    face_info['features'], 
                    name
                ):
                    print(f"âœ… æˆåŠŸæ·»åŠ  {name} åˆ°äººè„¸åº“!")
            else:
                print("âŒ å·²å–æ¶ˆ")
        else:
            print("è¯·é€‰æ‹©è¦ä¿å­˜çš„äººè„¸:")
            for i in range(len(captured_faces)):
                print(f"  [{i+1}] äººè„¸ {i+1}")
            print("  [0] å–æ¶ˆ")

            try:
                choice_input = input("é€‰æ‹© (0-{}): ".format(len(captured_faces))).strip()
                if not choice_input:
                    print("âŒ å·²å–æ¶ˆ")
                    self.input_mode = False
                    return
                choice = int(choice_input)

                if 1 <= choice <= len(captured_faces):
                    print("è¯·è¾“å…¥å§“å (ç›´æ¥å›è½¦å–æ¶ˆ): ", end='', flush=True)
                    name = input().strip()

                    if name:
                        face_info = captured_faces[choice - 1]
                        if self.face_database_manager.save_face_to_database(
                            face_info['crop'], 
                            face_info['features'], 
                            name
                        ):
                            print(f"âœ… æˆåŠŸæ·»åŠ  {name} åˆ°äººè„¸åº“!")
                    else:
                        print("âŒ å·²å–æ¶ˆ")
                else:
                    print("âŒ å·²å–æ¶ˆ")
            except (ValueError, EOFError, KeyboardInterrupt):
                print("âŒ æ“ä½œå·²å–æ¶ˆ")

        print()
        self.input_mode = False

    def run(self):
        """è¿è¡Œä¸»å¾ªç¯"""
        print("\nğŸ¬ å¼€å§‹å®æ—¶äººè„¸è¯†åˆ«...")
        print("=" * 60)
        print("âŒ¨ï¸  æŒ‰é”®è¯´æ˜:")
        print("   's'   - æ•è·å½“å‰äººè„¸å¹¶æ·»åŠ åˆ°æ•°æ®åº“")
        print("   'ESC' - å–æ¶ˆè¾“å…¥ (åœ¨è¾“å…¥å§“åæ—¶)")
        print("   'q'   - é€€å‡ºç¨‹åº")
        print("=" * 60)
        print(f"ğŸ“¹ è¾“å…¥æº: {self.input_source_type}")
        print(f"âš™ï¸  è·³å¸§æ£€æµ‹: æ¯ {self.detect_every_n_frames} å¸§æ‰§è¡Œä¸€æ¬¡æ£€æµ‹")
        print(f"ğŸ¯ åŸºå‡†é˜ˆå€¼: {self.base_threshold:.2f}")
        print(f"ğŸ§  è‡ªé€‚åº”é˜ˆå€¼: {'å¯ç”¨' if self.adaptive_threshold_enabled else 'ç¦ç”¨'}")
        print(f"ğŸ”§ å¤šçº¿ç¨‹å¤„ç†: å¯ç”¨")
        print()
        
        frame_id = 0
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºé™æ€å›¾ç‰‡æº
        is_static_image = self.input_handler.is_static_image_source()
        processed_static_image = False # æ ‡è®°é™æ€å›¾ç‰‡æ˜¯å¦å·²å¤„ç†å¹¶æ˜¾ç¤ºè¿‡ä¸€æ¬¡
        
        try:
            while True:
                loop_start = time.time()
                frame = self.input_handler.capture_frame()
                
                if frame is None:
                    print("âŒ æ— æ³•è·å–å¸§")
                    break
                
                self.frame_counter += 1
                frame_id += 1
                
                # å®šæœŸå‘æ£€æµ‹é˜Ÿåˆ—å‘é€å¸§
                # å¯¹äºé™æ€å›¾ç‰‡ï¼Œåªåœ¨ç¬¬ä¸€æ¬¡å¾ªç¯æ—¶å‘é€ä¸€æ¬¡
                if (self.frame_counter % self.detect_every_n_frames == 0 and not is_static_image) or \
                   (is_static_image and not processed_static_image):
                    if not self.detection_queue.full():
                        self.detection_queue.put((frame.copy(), frame_id))
                
                # ä»ç»“æœé˜Ÿåˆ—è·å–è¯†åˆ«ç»“æœ
                current_detections = []
                captured_faces_for_save = []
                
                # å°è¯•ä»é˜Ÿåˆ—è·å–æœ€æ–°ç»“æœï¼Œå¦‚æœé˜Ÿåˆ—ä¸ºç©ºåˆ™ä½¿ç”¨ä¸Šæ¬¡çš„ç»“æœ
                # å¯¹äºé™æ€å›¾ç‰‡ï¼Œéœ€è¦ç­‰å¾…è¯†åˆ«çº¿ç¨‹å®Œæˆ
                if is_static_image and not processed_static_image:
                    # å¯¹äºé™æ€å›¾ç‰‡ï¼Œç­‰å¾…è¯†åˆ«ç»“æœï¼Œç›´åˆ°é˜Ÿåˆ—ä¸­æœ‰æ•°æ®
                    try:
                        results, result_frame_id = self.result_queue.get(timeout=5) # å¢åŠ è¶…æ—¶
                        # print(f"Static image: Got results for frame_id {result_frame_id}")
                        for result in results:
                            bbox = result['bbox']
                            name = result['name']
                            confidence = result['confidence']
                            quality = result['quality']
                            
                            current_detections.append((bbox, name, confidence, quality))
                            
                            captured_faces_for_save.append({
                                'bbox': bbox,
                                'crop': result['crop'],
                                'features': result['features']
                            })
                        
                        self.last_detections = current_detections
                        self.last_captured_faces = captured_faces_for_save
                        processed_static_image = True # æ ‡è®°å·²å¤„ç†
                    except Exception as e:
                        print(f"âš ï¸ é™æ€å›¾ç‰‡è¯†åˆ«è¶…æ—¶æˆ–é”™è¯¯: {e}. æ˜¾ç¤ºåŸå§‹å›¾ç‰‡ã€‚")
                        current_detections = [] # å¦‚æœè¶…æ—¶ï¼Œä¸æ˜¾ç¤ºä»»ä½•æ£€æµ‹æ¡†
                elif not self.result_queue.empty():
                    try:
                        results, result_frame_id = self.result_queue.get_nowait()
                        
                        for result in results:
                            bbox = result['bbox']
                            name = result['name']
                            confidence = result['confidence']
                            quality = result['quality']
                            
                            current_detections.append((bbox, name, confidence, quality))
                            
                            captured_faces_for_save.append({
                                'bbox': bbox,
                                'crop': result['crop'],
                                'features': result['features']
                            })
                        
                        self.last_detections = current_detections
                        self.last_captured_faces = captured_faces_for_save
                        
                    except:
                        current_detections = self.last_detections
                else:
                    current_detections = self.last_detections

                # ç»˜åˆ¶æ£€æµ‹æ¡†å’Œä¿¡æ¯
                for bbox, name, confidence, quality in current_detections:
                    x1, y1, x2, y2 = bbox
                    
                    # æ ¹æ®è¯†åˆ«ç»“æœé€‰æ‹©é¢œè‰²
                    if name != "Unknown":
                        color_rgb = (0, 255, 0)
                    else:
                        color_rgb = (255, 0, 0)
                    
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color_rgb, 2)
                    
                    # æ˜¾ç¤ºè¯†åˆ«ç»“æœ
                    label = f"{name}: {confidence:.1f}%"
                    cv2.putText(frame, label, (x1, y1 - 10),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color_rgb, 2)
                    
                    # æ˜¾ç¤ºè´¨é‡æŒ‡æ ‡ï¼ˆè°ƒè¯•ä¿¡æ¯ï¼‰
                    quality_color = get_quality_indicator_color(quality)
                    quality_text = f"Q: B{quality['brightness']:.0f} C{quality['contrast']:.0f} S{quality['blur']:.0f}"
                    cv2.putText(frame, quality_text, (x1, y2 + 20),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, quality_color, 1)

                if self.input_mode:
                    mode_text = "Waiting for input... (Check terminal)"
                    cv2.putText(frame, mode_text, (10, frame.shape[0] - 20),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)

                # æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
                self.update_fps()
                cv2.putText(frame, f"FPS: {self.fps:.1f}", (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
                cv2.putText(frame, f"Database: {len(self.face_database_manager.face_database)} faces", 
                           (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                cv2.putText(frame, f"Detected: {len(current_detections)} face(s)", 
                           (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                # æ˜¾ç¤ºè‡ªé€‚åº”é˜ˆå€¼ä¿¡æ¯
                if self.adaptive_threshold_enabled:
                    threshold_text = f"Threshold: {self.quality_assessor.current_threshold:.3f} (Base: {self.base_threshold:.2f})"
                    cv2.putText(frame, threshold_text, (10, 120),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 0), 1)

                draw_help_text(frame)

                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                cv2.imshow("Face Recognition System - Multi-threaded", frame_bgr)

                # åŠ¨æ€è°ƒæ•´å»¶è¿Ÿ
                if is_static_image and processed_static_image:
                    key = cv2.waitKey(0) & 0xFF 
                    if key == ord('q'):
                        break
                    elif key == ord('s') and not self.input_mode:
                        captured_faces_snapshot = self.last_captured_faces.copy()
                        if not captured_faces_snapshot:
                            print("âŒ æœªæ£€æµ‹åˆ°äººè„¸ï¼Œè¯·å¯¹å‡†æ‘„åƒå¤´")
                            continue

                        self.input_mode = True
                        thread = threading.Thread(
                            target=self.handle_face_capture_async,
                            args=(captured_faces_snapshot,)
                        )
                        thread.daemon = True
                        thread.start()
                    continue
                elif is_static_image and not processed_static_image:
                    delay = 1 
                else:
                    elapsed = time.time() - loop_start
                    target_fps = 24
                    delay = max(1, int((1.0 / target_fps - elapsed) * 1000))
                
                key = cv2.waitKey(delay) & 0xFF

                if key == ord('q'):
                    break
                elif key == ord('s') and not self.input_mode:
                    captured_faces_snapshot = self.last_captured_faces.copy()
                    if not captured_faces_snapshot:
                        print("âŒ æœªæ£€æµ‹åˆ°äººè„¸ï¼Œè¯·å¯¹å‡†æ‘„åƒå¤´")
                        continue

                    self.input_mode = True
                    thread = threading.Thread(
                        target=self.handle_face_capture_async,
                        args=(captured_faces_snapshot,)
                    )
                    thread.daemon = True
                    thread.start()

        except KeyboardInterrupt:
            print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­")
        finally:
            self.cleanup()
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        print("ğŸ§¹ æ¸…ç†èµ„æº...")
        self.running = False
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        if self.detection_thread and self.detection_thread.is_alive():
            self.detection_thread.join(timeout=1)
        if self.recognition_thread and self.recognition_thread.is_alive():
            self.recognition_thread.join(timeout=1)
        
        self.input_handler.release()
        cv2.destroyAllWindows()
        print("âœ… å®Œæˆ")
